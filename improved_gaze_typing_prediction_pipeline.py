# -*- coding: utf-8 -*-
"""Improved Gaze-Typing Prediction Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Q2wh881UMn_x3wE1uYPrd9IMDd7ss7U
"""

!gdown 1Vs6xp-3PXXyuwWFcN1IeRExGifTlyFSC
!gdown 1dzVCeytBa1NtkGQHIRPAhRsDfWCqp8N3
!gdown 1FPJn-gZepvgm5IofoAMvtqeyrUVQ4UPa
!gdown 1enYInARYoh3SYXSwCpejZ9YYZMpdrhkW
!gdown 1Xryic0ICQsKdqV1hsysx6tzF_2bZvt4N

!pip install rarfile

# First, install the 'unrar' tool which is required by the rarfile library.
# This command is for Debian/Ubuntu-based systems (like Google Colab).
# It only needs to be run once.
try:
    import rarfile
    print("rarfile library is already installed.")
except ImportError:
    print("Installing rarfile...")
    !pip install rarfile

# Install unrar tool (required for rarfile to work)
!apt-get install unrar -y

import os
import rarfile
import glob

# Set the path to the unrar tool. This is a common path on Linux systems.
rarfile.UNRAR_TOOL = "/usr/bin/unrar"

# --- CONFIGURATION ---
# Define the list of RAR files to extract.
# You can use glob to find them automatically if they follow a pattern.
# For example: rar_files = glob.glob("/content/*.rar")
rar_files = ["/content/participant_1.rar", "/content/participant_2.rar", "/content/participant_3.rar", "/content/participant_4.rar", "/content/participant_5.rar"]

# Define the base directory where all the extracted folders will be placed.
base_extract_dir = "/content/extracted_participants"

# --- SCRIPT EXECUTION ---
def extract_rar_files(file_list, output_dir):
    """
    Extracts a list of RAR files into subdirectories within a specified output directory.

    Args:
        file_list (list): A list of paths to the RAR files.
        output_dir (str): The path to the base directory for extraction.
    """
    if not file_list:
        print("No RAR files to extract. Please check the file paths.")
        return

    # Create the base extraction directory if it doesn't exist.
    os.makedirs(output_dir, exist_ok=True)
    print(f"Created base directory: {output_dir}")

    # Iterate through each RAR file in the list.
    for rar_path in file_list:
        if not os.path.exists(rar_path):
            print(f"Warning: File not found at {rar_path}. Skipping.")
            continue

        try:
            # Get the participant name from the filename (e.g., 'participant_1').
            participant_name = os.path.splitext(os.path.basename(rar_path))[0]

            # Create a separate folder for each participant's data.
            extract_path = os.path.join(output_dir, participant_name)
            os.makedirs(extract_path, exist_ok=True)

            print(f"\nExtracting '{rar_path}' to '{extract_path}'...")

            # Open the RAR file and extract all contents to the new directory.
            with rarfile.RarFile(rar_path) as rf:
                rf.extractall(path=extract_path)

            # List some of the extracted files to confirm success.
            files = os.listdir(extract_path)
            print(f"Extraction successful. Extracted {len(files)} files.")
            print(f"Sample files from '{participant_name}':", files[:5])

        except rarfile.RarCannotExec as e:
            print(f"Error: The unrar tool could not be executed. Please ensure it's installed and the path '{rarfile.UNRAR_TOOL}' is correct.")
            print(e)
            break
        except rarfile.BadRarFile as e:
            print(f"Error: The file '{rar_path}' is not a valid RAR file.")
            print(e)
            continue
        except Exception as e:
            print(f"An unexpected error occurred while processing '{rar_path}':")
            print(e)
            continue

if __name__ == "__main__":
    extract_rar_files(rar_files, base_extract_dir)

import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import joblib

# --- CONFIG ---
MAX_SEQUENCE_LENGTH = 50
EPOCHS = 40
BATCH_SIZE = 32
PATIENCE = 5 # For EarlyStopping

INPUT_FOLDER = "/content/extracted_participants" # Updated to reflect your extraction path

# --- SCRIPT FUNCTIONS ---

def calculate_distance(df, p1_prefix, p2_prefix, new_col_name):
    """Calculates the Euclidean distance between two points (p1 and p2) across a DataFrame."""
    df[new_col_name] = np.sqrt(
        (df[f'{p1_prefix}_X'] - df[f'{p2_prefix}_X'])**2 +
        (df[f'{p1_prefix}_Y'] - df[f'{p2_prefix}_Y'])**2 +
        (df[f'{p1_prefix}_Z'] - df[f'{p2_prefix}_Z'])**2
    )
    return df

def load_and_preprocess_data(folder_path):
    """
    Loads all data, handles missing values, and performs dynamic feature engineering.

    This version now uses a more robust approach to prevent key errors and
    performance warnings by building all new features in a single pass before
    concatenating them to the main DataFrame.
    """
    print("Step 1: Loading and preprocessing data...")
    all_csv_files = glob.glob(os.path.join(folder_path, "**/*.csv"), recursive=True)
    if not all_csv_files:
        print("Error: No CSV files found.")
        return None

    # Concatenate all files into a single DataFrame
    aggregated_df = pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index=True)
    print(f"Aggregated data loaded with {len(aggregated_df)} rows.")

    # Handle missing values with interpolation for all numerical columns
    print("Interpolating missing data...")
    numerical_cols = aggregated_df.select_dtypes(include=np.number).columns
    aggregated_df[numerical_cols] = aggregated_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[numerical_cols].transform(lambda x: x.interpolate(limit_direction='both'))

    # Drop rows that still have NaNs (e.g., at the beginning of a sequence)
    aggregated_df.dropna(inplace=True)

    # Store the original DataFrame to add new columns to later
    df = aggregated_df.copy()

    # Dictionary to hold all newly engineered features
    new_features = {}

    print("Step 2: Performing dynamic feature normalization...")

    # Use the average of the two wrist roots as the dynamic origin for each row.
    df['Origin_X'] = (df['Left_Hand_WristRoot_X'] + df['Right_Hand_WristRoot_X']) / 2
    df['Origin_Y'] = (df['Left_Hand_WristRoot_Y'] + df['Right_Hand_WristRoot_Y']) / 2
    df['Origin_Z'] = (df['Left_Hand_WristRoot_Z'] + df['Right_Hand_WristRoot_Z']) / 2

    # Identify all original coordinate columns that need normalization
    # This now filters out any already-normalized columns to prevent the KeyError.
    raw_coord_cols = [col for col in df.columns if ('_X' in col or '_Y' in col or '_Z' in col) and 'GazeRay' not in col and not col.startswith('normalized_')]
    key_cols = [col for col in raw_coord_cols if col.startswith('Key_')]

    # Create normalized columns in a single vectorized operation and add to new_features dict
    for col in raw_coord_cols:
        new_features[f'normalized_{col}'] = df[col] - df[f'Origin_{col[-1]}']

    # Add the origin columns to the DataFrame since they're needed for normalization
    df = df.assign(
        Origin_X = new_features.get('Origin_X', df['Origin_X']),
        Origin_Y = new_features.get('Origin_Y', df['Origin_Y']),
        Origin_Z = new_features.get('Origin_Z', df['Origin_Z'])
    )

    print("Step 3: Inferring the pressed finger based on proximity...")

    # Use the newly created normalized columns for inference
    normalized_df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    finger_tip_cols_x = [col for col in normalized_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    # Get a list of unique keys that actually have corresponding columns in the DataFrame
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in normalized_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]
    unique_pressed_letters = normalized_df['PressedLetter'].dropna().unique()

    # Filter the list of pressed letters to only include those with available key data
    processed_keys = [key for key in unique_pressed_letters if key in available_keys]

    all_distances = pd.DataFrame(index=normalized_df.index, columns=finger_tip_names, dtype=float)

    # UPDATED: Use 3D coordinates for proximity calculation
    for key in processed_keys:
        mask = (normalized_df['PressedLetter'] == key)
        if not mask.any():
            continue

        key_coords = normalized_df.loc[mask, [f'normalized_Key_{key}_X', f'normalized_Key_{key}_Y', f'normalized_Key_{key}_Z']].values

        for finger in finger_tip_names:
            finger_coords = normalized_df.loc[mask, [f'normalized_{finger}_X', f'normalized_{finger}_Y', f'normalized_{finger}_Z']].values
            dist_sq = ((finger_coords - key_coords)**2).sum(axis=1)
            all_distances.loc[mask, finger] = dist_sq

    # Use .idxmin with skipna=True to avoid the FutureWarning
    df['InferredPressedFinger'] = all_distances.idxmin(axis=1, skipna=True)
    df.dropna(subset=['InferredPressedFinger'], inplace=True)

    # Rebuild new_features dictionary with normalized and inferred data for the next steps
    temp_df = pd.concat([df, pd.DataFrame(new_features)], axis=1)
    normalized_cols = [col for col in temp_df.columns if col.startswith('normalized_')]

    print("Step 4: Creating delta features from normalized data...")
    for col in normalized_cols:
        new_features[f'delta_{col}'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[col].diff().fillna(0)

    print("Step 5: Engineering new geometric and kinematic features...")

    # Kinematic features: Hand velocity
    new_features['Left_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Left_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))
    new_features['Right_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Right_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))

    # Gaze-to-key distance features
    normalized_gaze_x = 'normalized_LeftGazeHitPosition_X'
    normalized_gaze_y = 'normalized_LeftGazeHitPosition_Y'
    normalized_gaze_z = 'normalized_LeftGazeHitPosition_Z' # ADDED
    unique_keys = [col.split('_')[1] for col in key_cols if col.endswith('_X')]
    for key in unique_keys:
        key_x_col = f'normalized_Key_{key}_X'
        key_y_col = f'normalized_Key_{key}_Y'
        key_z_col = f'normalized_Key_{key}_Z' # ADDED
        if key_x_col in temp_df.columns and key_y_col in temp_df.columns and key_z_col in temp_df.columns: # MODIFIED check
            new_features[f'gaze_dist_{key}'] = np.sqrt(
                (temp_df[normalized_gaze_x] - temp_df[key_x_col])**2 +
                (temp_df[normalized_gaze_y] - temp_df[key_y_col])**2 +
                (temp_df[normalized_gaze_z] - temp_df[key_z_col])**2 # ADDED Z component
            )

    # Finger-to-key distance features (Vectorized and robust)
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in temp_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]

    pressed_key_x_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_y_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_z_series = pd.Series(index=temp_df.index, dtype=float) # ADDED

    for key in available_keys:
        mask = temp_df['PressedLetter'] == key
        if mask.any():
            pressed_key_x_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_X']
            pressed_key_y_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Y']
            pressed_key_z_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Z'] # ADDED

    finger_tip_cols_x = [col for col in temp_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    for finger in finger_tip_names:
        finger_x_col = f'normalized_{finger}_X'
        finger_y_col = f'normalized_{finger}_Y'
        finger_z_col = f'normalized_{finger}_Z' # ADDED

        distances = np.sqrt(
            (temp_df[finger_x_col] - pressed_key_x_series)**2 +
            (temp_df[finger_y_col] - pressed_key_y_series)**2 +
            (temp_df[finger_z_col] - pressed_key_z_series)**2 # ADDED Z component
        )
        new_features[f'finger_dist_{finger}'] = distances

    # Concatenate all new features at once to the original df
    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    df.dropna(inplace=True)
    print("Data preprocessing complete.")
    return df

def create_padded_sequences(df, features, target_col):
    """
    Groups data and pads sequences for LSTM input.
    """
    sequences, labels, participants = [], [], []

    max_len = 0
    unique_groups = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])
    for _, group in unique_groups:
        max_len = max(max_len, len(group))

    if max_len > MAX_SEQUENCE_LENGTH:
        max_len = MAX_SEQUENCE_LENGTH

    for group_key, group in unique_groups:
        group_features = group[features].values
        padded_features = pad_sequences([group_features], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0]

        sequences.append(padded_features)
        labels.append(group[target_col].iloc[-1])
        participants.append(group_key[0])

    return np.array(sequences), np.array(labels), np.array(participants)

def build_model(input_shape, num_classes):
    """
    Builds and compiles the Bidirectional LSTM model with a Masking layer.
    """
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=input_shape))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def run_loocv_evaluation(df, features, target_col, stage_name):
    """
    Performs Leave-One-Out Cross-Validation on the provided data and returns metrics.
    """
    print(f"\n--- Running LOOCV for {stage_name} ---")

    X_sequences, y_labels, participant_ids = create_padded_sequences(df, features, target_col)

    if len(X_sequences) == 0:
      print("Skipping evaluation: No data available for this stage.")
      return None

    unique_participants = np.unique(participant_ids)
    participant_map = {p: i for i, p in enumerate(unique_participants)}
    participant_indices = np.array([participant_map[p] for p in participant_ids])

    if len(unique_participants) < 2:
        print("Skipping evaluation: Not enough participants for LOOCV.")
        return None

    le = LabelEncoder()
    y_encoded = le.fit_transform(y_labels)
    num_classes = len(le.classes_)
    y_categorical = to_categorical(y_encoded, num_classes=num_classes)

    print(f"Total samples for evaluation: {X_sequences.shape[0]}. Classes: {le.classes_}")

    loocv = LeaveOneOut()
    accuracy_scores, y_true_all, y_pred_all = [], [], []

    for train_participant_indices, test_participant_indices in loocv.split(unique_participants):
        train_participants = unique_participants[train_participant_indices]
        test_participant = unique_participants[test_participant_indices][0]

        train_indices = np.where(np.isin(participant_ids, train_participants))[0]
        test_indices = np.where(participant_ids == test_participant)[0]

        X_train, X_test = X_sequences[train_indices], X_sequences[test_indices]
        y_train, y_test = y_categorical[train_indices], y_categorical[test_indices]
        y_test_labels = np.argmax(y_test, axis=1)

        model = build_model(X_train.shape[1:], num_classes)
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)

        class_counts = pd.Series(y_train.argmax(axis=1)).value_counts()
        total_samples = len(y_train)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}

        history = model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            class_weight=class_weights
        )

        y_pred_probs = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracy_scores.append(accuracy)

        y_true_all.extend(y_test_labels)
        y_pred_all.extend(y_pred)

    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    final_report = classification_report(y_true_all, y_pred_all, target_names=le.classes_, zero_division=0)

    return avg_accuracy, final_report, le, y_true_all, y_pred_all

# --- MAIN SCRIPT EXECUTION ---
if __name__ == "__main__":

    full_df = load_and_preprocess_data(INPUT_FOLDER)
    if full_df is None:
        exit()

    # --- Stage 1: Finger Prediction ---
    stage1_features = [
        col for col in full_df.columns if (
            col.startswith('normalized_') or
            col.startswith('delta_') or
            col.startswith('Left_Hand_WristRoot_Velocity') or
            col.startswith('Right_Hand_WristRoot_Velocity')
        ) and 'Key_' not in col and 'Gaze' not in col
    ]
    avg_stage1_accuracy, stage1_report, le_stage1, _, _ = run_loocv_evaluation(
        full_df,
        stage1_features,
        'InferredPressedFinger',
        "Stage 1: Finger Prediction"
    )

    print("\n==============================================")
    print("Stage 1 LOOCV Results: Finger Prediction")
    print(f"Average Accuracy: {avg_stage1_accuracy:.4f}")
    print("\nClassification Report:")
    print(stage1_report)
    print("==============================================\n")

    # --- Stage 2: Character Prediction within each Finger ---
    finger_accuracies = {}
    unique_fingers = sorted(full_df['InferredPressedFinger'].unique())

    print("Stage 2: Character Prediction (LOOCV for each finger)")
    for finger_id in unique_fingers:
        finger_df = full_df[full_df['InferredPressedFinger'] == finger_id].copy()

        stage2_features = [
            col for col in finger_df.columns if (
                col.startswith('normalized_') or
                col.startswith('delta_') or
                col.startswith('gaze_dist_') or
                col.startswith('finger_dist_') or
                col.startswith('Left_Hand_WristRoot_Velocity') or
                col.startswith('Right_Hand_WristRoot_Velocity')
            )
        ]

        result = run_loocv_evaluation(
            finger_df,
            stage2_features,
            'PressedLetter',
            f"Stage 2, Finger {finger_id}"
        )

        if result:
            avg_accuracy, report, le, y_true, y_pred = result
            finger_accuracies[finger_id] = {
                'accuracy': avg_accuracy,
                'report': report,
                'le': le,
                'y_true': y_true,
                'y_pred': y_pred
            }
            print(f"\nAverage Accuracy for Finger {finger_id}: {avg_accuracy:.4f}")
            print(f"Classification Report for Finger {finger_id}:\n{report}")

    print("\n==============================================")
    print("Final Stage 2 Accuracy Report")
    for finger_id, metrics in finger_accuracies.items():
        print(f"Inferred Finger {finger_id} (Keys: {list(metrics['le'].classes_)}): {metrics['accuracy']:.4f}")
    print("==============================================")

import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import joblib

# --- CONFIG ---
MAX_SEQUENCE_LENGTH = 50
EPOCHS = 40
BATCH_SIZE = 32
PATIENCE = 5 # For EarlyStopping

INPUT_FOLDER = "/content/extracted_participants" # Updated to reflect your extraction path

# --- SCRIPT FUNCTIONS ---

def calculate_distance(df, p1_prefix, p2_prefix, new_col_name):
    """Calculates the Euclidean distance between two points (p1 and p2) across a DataFrame."""
    df[new_col_name] = np.sqrt(
        (df[f'{p1_prefix}_X'] - df[f'{p2_prefix}_X'])**2 +
        (df[f'{p1_prefix}_Y'] - df[f'{p2_prefix}_Y'])**2 +
        (df[f'{p1_prefix}_Z'] - df[f'{p2_prefix}_Z'])**2
    )
    return df

def load_and_preprocess_data(folder_path):
    """
    Loads all data, handles missing values, and performs dynamic feature engineering.

    This version now uses a more robust approach to prevent key errors and
    performance warnings by building all new features in a single pass before
    concatenating them to the main DataFrame.
    """
    print("Step 1: Loading and preprocessing data...")
    all_csv_files = glob.glob(os.path.join(folder_path, "**/*.csv"), recursive=True)
    if not all_csv_files:
        print("Error: No CSV files found.")
        return None

    # Concatenate all files into a single DataFrame
    aggregated_df = pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index=True)
    print(f"Aggregated data loaded with {len(aggregated_df)} rows.")

    # NEW STEP: Remove rows where there is a mismatch between PressedLetter and CurrentLetter (fixed column name)
    print("Removing rows with typos (where PressedLetter != CurrentLetter)...")
    aggregated_df = aggregated_df[aggregated_df['PressedLetter'] == aggregated_df['CurrentLetter']].copy()
    print(f"Remaining rows after typo removal: {len(aggregated_df)}")

    if len(aggregated_df) == 0:
      print("No valid data remaining after typo removal. Please check your dataset.")
      return None

    # Handle missing values with interpolation for all numerical columns
    print("Interpolating missing data...")
    numerical_cols = aggregated_df.select_dtypes(include=np.number).columns
    aggregated_df[numerical_cols] = aggregated_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[numerical_cols].transform(lambda x: x.interpolate(limit_direction='both'))

    # Drop rows that still have NaNs (e.g., at the beginning of a sequence)
    aggregated_df.dropna(inplace=True)

    # Store the original DataFrame to add new columns to later
    df = aggregated_df.copy()

    # Dictionary to hold all newly engineered features
    new_features = {}

    print("Step 2: Performing dynamic feature normalization...")

    # Use the average of the two wrist roots as the dynamic origin for each row.
    df['Origin_X'] = (df['Left_Hand_WristRoot_X'] + df['Right_Hand_WristRoot_X']) / 2
    df['Origin_Y'] = (df['Left_Hand_WristRoot_Y'] + df['Right_Hand_WristRoot_Y']) / 2
    df['Origin_Z'] = (df['Left_Hand_WristRoot_Z'] + df['Right_Hand_WristRoot_Z']) / 2

    # Identify all original coordinate columns that need normalization
    # This now filters out any already-normalized columns to prevent the KeyError.
    raw_coord_cols = [col for col in df.columns if ('_X' in col or '_Y' in col or '_Z' in col) and 'GazeRay' not in col and not col.startswith('normalized_')]
    key_cols = [col for col in raw_coord_cols if col.startswith('Key_')]

    # Create normalized columns in a single vectorized operation and add to new_features dict
    for col in raw_coord_cols:
        new_features[f'normalized_{col}'] = df[col] - df[f'Origin_{col[-1]}']

    # Add the origin columns to the DataFrame since they're needed for normalization
    df = df.assign(
        Origin_X = new_features.get('Origin_X', df['Origin_X']),
        Origin_Y = new_features.get('Origin_Y', df['Origin_Y']),
        Origin_Z = new_features.get('Origin_Z', df['Origin_Z'])
    )

    print("Step 3: Inferring the pressed finger based on proximity...")

    # Use the newly created normalized columns for inference
    normalized_df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    finger_tip_cols_x = [col for col in normalized_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    # Get a list of unique keys that actually have corresponding columns in the DataFrame
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in normalized_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]
    unique_pressed_letters = normalized_df['PressedLetter'].dropna().unique()

    # Filter the list of pressed letters to only include those with available key data
    processed_keys = [key for key in unique_pressed_letters if key in available_keys]

    all_distances = pd.DataFrame(index=normalized_df.index, columns=finger_tip_names, dtype=float)

    # UPDATED: Use 3D coordinates for proximity calculation
    for key in processed_keys:
        mask = (normalized_df['PressedLetter'] == key)
        if not mask.any():
            continue

        key_coords = normalized_df.loc[mask, [f'normalized_Key_{key}_X', f'normalized_Key_{key}_Y', f'normalized_Key_{key}_Z']].values

        for finger in finger_tip_names:
            finger_coords = normalized_df.loc[mask, [f'normalized_{finger}_X', f'normalized_{finger}_Y', f'normalized_{finger}_Z']].values
            dist_sq = ((finger_coords - key_coords)**2).sum(axis=1)
            all_distances.loc[mask, finger] = dist_sq

    # Use .idxmin with skipna=True to avoid the FutureWarning
    df['InferredPressedFinger'] = all_distances.idxmin(axis=1, skipna=True)
    df.dropna(subset=['InferredPressedFinger'], inplace=True)

    # Rebuild new_features dictionary with normalized and inferred data for the next steps
    temp_df = pd.concat([df, pd.DataFrame(new_features)], axis=1)
    normalized_cols = [col for col in temp_df.columns if col.startswith('normalized_')]

    print("Step 4: Creating delta features from normalized data...")
    for col in normalized_cols:
        new_features[f'delta_{col}'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[col].diff().fillna(0)

    print("Step 5: Engineering new geometric and kinematic features...")

    # Kinematic features: Hand velocity
    new_features['Left_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Left_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))
    new_features['Right_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Right_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))

    # Gaze-to-key distance features - IMPROVED to handle both Left and Right Gaze
    unique_keys = [col.split('_')[1] for col in key_cols if col.endswith('_X')]

    temp_df['GazeHitPosition_X'] = np.nan
    temp_df['GazeHitPosition_Y'] = np.nan
    temp_df['GazeHitPosition_Z'] = np.nan

    # Case 1: Both left and right gaze hit flags are 1 (using corrected column names)
    both_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 1)
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_X'] = (temp_df['LeftGazeHitPosition_X'] + temp_df['RightGazeHitPosition_X']) / 2
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_Y'] = (temp_df['LeftGazeHitPosition_Y'] + temp_df['RightGazeHitPosition_Y']) / 2
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_Z'] = (temp_df['LeftGazeHitPosition_Z'] + temp_df['RightGazeHitPosition_Z']) / 2

    # Case 2: Only left gaze hit flag is 1 (using corrected column names)
    left_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 0)
    temp_df.loc[left_gaze_hit, ['GazeHitPosition_X', 'GazeHitPosition_Y', 'GazeHitPosition_Z']] = temp_df.loc[left_gaze_hit, ['LeftGazeHitPosition_X', 'LeftGazeHitPosition_Y', 'LeftGazeHitPosition_Z']].values

    # Case 3: Only right gaze hit flag is 1 (using corrected column names)
    right_gaze_hit = (temp_df['LeftGazeHit'] == 0) & (temp_df['RightGazeHit'] == 1)
    temp_df.loc[right_gaze_hit, ['GazeHitPosition_X', 'GazeHitPosition_Y', 'GazeHitPosition_Z']] = temp_df.loc[right_gaze_hit, ['RightGazeHitPosition_X', 'RightGazeHitPosition_Y', 'RightGazeHitPosition_Z']].values

    for key in unique_keys:
        key_x_col = f'normalized_Key_{key}_X'
        key_y_col = f'normalized_Key_{key}_Y'
        key_z_col = f'normalized_Key_{key}_Z'
        if key_x_col in temp_df.columns and key_y_col in temp_df.columns and key_z_col in temp_df.columns:
            new_features[f'gaze_dist_{key}'] = np.sqrt(
                (temp_df['GazeHitPosition_X'] - temp_df[key_x_col])**2 +
                (temp_df['GazeHitPosition_Y'] - temp_df[key_y_col])**2 +
                (temp_df['GazeHitPosition_Z'] - temp_df[key_z_col])**2
            )

    # Finger-to-key distance features (Vectorized and robust)
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in temp_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]

    pressed_key_x_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_y_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_z_series = pd.Series(index=temp_df.index, dtype=float)

    for key in available_keys:
        mask = temp_df['PressedLetter'] == key
        if mask.any():
            pressed_key_x_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_X']
            pressed_key_y_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Y']
            pressed_key_z_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Z']

    finger_tip_cols_x = [col for col in temp_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    for finger in finger_tip_names:
        finger_x_col = f'normalized_{finger}_X'
        finger_y_col = f'normalized_{finger}_Y'
        finger_z_col = f'normalized_{finger}_Z'

        distances = np.sqrt(
            (temp_df[finger_x_col] - pressed_key_x_series)**2 +
            (temp_df[finger_y_col] - pressed_key_y_series)**2 +
            (temp_df[finger_z_col] - pressed_key_z_series)**2
        )
        new_features[f'finger_dist_{finger}'] = distances

    # Concatenate all new features at once to the original df
    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    df.dropna(inplace=True)
    print("Data preprocessing complete.")
    return df

def create_padded_sequences(df, features, target_col):
    """
    Groups data and pads sequences for LSTM input.
    """
    sequences, labels, participants = [], [], []

    max_len = 0
    unique_groups = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])
    for _, group in unique_groups:
        max_len = max(max_len, len(group))

    if max_len > MAX_SEQUENCE_LENGTH:
        max_len = MAX_SEQUENCE_LENGTH

    for group_key, group in unique_groups:
        group_features = group[features].values
        padded_features = pad_sequences([group_features], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0]

        sequences.append(padded_features)
        labels.append(group[target_col].iloc[-1])
        participants.append(group_key[0])

    return np.array(sequences), np.array(labels), np.array(participants)

def build_model(input_shape, num_classes):
    """
    Builds and compiles the Bidirectional LSTM model with a Masking layer.
    """
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=input_shape))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def run_loocv_evaluation(df, features, target_col, stage_name):
    """
    Performs Leave-One-Out Cross-Validation on the provided data and returns metrics.
    """
    print(f"\n--- Running LOOCV for {stage_name} ---")

    X_sequences, y_labels, participant_ids = create_padded_sequences(df, features, target_col)

    if len(X_sequences) == 0:
      print("Skipping evaluation: No data available for this stage.")
      return None

    unique_participants = np.unique(participant_ids)
    participant_map = {p: i for i, p in enumerate(unique_participants)}
    participant_indices = np.array([participant_map[p] for p in participant_ids])

    if len(unique_participants) < 2:
        print("Skipping evaluation: Not enough participants for LOOCV.")
        return None

    le = LabelEncoder()
    y_encoded = le.fit_transform(y_labels)
    num_classes = len(le.classes_)
    y_categorical = to_categorical(y_encoded, num_classes=num_classes)

    print(f"Total samples for evaluation: {X_sequences.shape[0]}. Classes: {le.classes_}")

    loocv = LeaveOneOut()
    accuracy_scores, y_true_all, y_pred_all = [], [], []

    for train_participant_indices, test_participant_indices in loocv.split(unique_participants):
        train_participants = unique_participants[train_participant_indices]
        test_participant = unique_participants[test_participant_indices][0]

        train_indices = np.where(np.isin(participant_ids, train_participants))[0]
        test_indices = np.where(participant_ids == test_participant)[0]

        X_train, X_test = X_sequences[train_indices], X_sequences[test_indices]
        y_train, y_test = y_categorical[train_indices], y_categorical[test_indices]
        y_test_labels = np.argmax(y_test, axis=1)

        model = build_model(X_train.shape[1:], num_classes)
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)

        class_counts = pd.Series(y_train.argmax(axis=1)).value_counts()
        total_samples = len(y_train)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}

        history = model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            class_weight=class_weights
        )

        y_pred_probs = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracy_scores.append(accuracy)

        y_true_all.extend(y_test_labels)
        y_pred_all.extend(y_pred)

    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    final_report = classification_report(y_true_all, y_pred_all, target_names=le.classes_, zero_division=0)

    return avg_accuracy, final_report, le, y_true_all, y_pred_all

# --- MAIN SCRIPT EXECUTION ---
if __name__ == "__main__":

    full_df = load_and_preprocess_data(INPUT_FOLDER)
    if full_df is None:
        exit()

    # --- Stage 1: Finger Prediction ---
    stage1_features = [
        col for col in full_df.columns if (
            col.startswith('normalized_') or
            col.startswith('delta_') or
            col.startswith('Left_Hand_WristRoot_Velocity') or
            col.startswith('Right_Hand_WristRoot_Velocity')
        ) and 'Key_' not in col and 'Gaze' not in col
    ]
    avg_stage1_accuracy, stage1_report, le_stage1, _, _ = run_loocv_evaluation(
        full_df,
        stage1_features,
        'InferredPressedFinger',
        "Stage 1: Finger Prediction"
    )

    print("\n==============================================")
    print("Stage 1 LOOCV Results: Finger Prediction")
    print(f"Average Accuracy: {avg_stage1_accuracy:.4f}")
    print("\nClassification Report:")
    print(stage1_report)
    print("==============================================\n")

    # --- Stage 2: Character Prediction within each Finger ---
    finger_accuracies = {}
    unique_fingers = sorted(full_df['InferredPressedFinger'].unique())

    print("Stage 2: Character Prediction (LOOCV for each finger)")
    for finger_id in unique_fingers:
        finger_df = full_df[full_df['InferredPressedFinger'] == finger_id].copy()

        stage2_features = [
            col for col in finger_df.columns if (
                col.startswith('normalized_') or
                col.startswith('delta_') or
                col.startswith('gaze_dist_') or
                col.startswith('finger_dist_') or
                col.startswith('Left_Hand_WristRoot_Velocity') or
                col.startswith('Right_Hand_WristRoot_Velocity')
            )
        ]

        result = run_loocv_evaluation(
            finger_df,
            stage2_features,
            'PressedLetter',
            f"Stage 2, Finger {finger_id}"
        )

        if result:
            avg_accuracy, report, le, y_true, y_pred = result
            finger_accuracies[finger_id] = {
                'accuracy': avg_accuracy,
                'report': report,
                'le': le,
                'y_true': y_true,
                'y_pred': y_pred
            }
            print(f"\nAverage Accuracy for Finger {finger_id}: {avg_accuracy:.4f}")
            print(f"Classification Report for Finger {finger_id}:\n{report}")

    print("\n==============================================")
    print("Final Stage 2 Accuracy Report")
    for finger_id, metrics in finger_accuracies.items():
        print(f"Inferred Finger {finger_id} (Keys: {list(metrics['le'].classes_)}): {metrics['accuracy']:.4f}")
    print("==============================================")











# Cell 1: Imports and Configuration

import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import joblib

# --- CONFIG ---
MAX_SEQUENCE_LENGTH = 50
EPOCHS = 40
BATCH_SIZE = 32
PATIENCE = 5 # For EarlyStopping

INPUT_FOLDER = "/content/extracted_participants" # Updated to reflect your extraction path

# Cell 2: Script Functions

def calculate_distance(df, p1_prefix, p2_prefix, new_col_name):
    """Calculates the Euclidean distance between two points (p1 and p2) across a DataFrame."""
    df[new_col_name] = np.sqrt(
        (df[f'{p1_prefix}_X'] - df[f'{p2_prefix}_X'])**2 +
        (df[f'{p1_prefix}_Y'] - df[f'{p2_prefix}_Y'])**2 +
        (df[f'{p1_prefix}_Z'] - df[f'{p2_prefix}_Z'])**2
    )
    return df

def load_and_preprocess_data(folder_path):
    """
    Loads all data, handles missing values, and performs dynamic feature engineering.

    This version now uses a more robust approach to prevent key errors and
    performance warnings by building all new features in a single pass before
    concatenating them to the main DataFrame.
    """
    print("Step 1: Loading and preprocessing data...")
    all_csv_files = glob.glob(os.path.join(folder_path, "**/*.csv"), recursive=True)
    if not all_csv_files:
        print("Error: No CSV files found.")
        return None

    # Concatenate all files into a single DataFrame
    aggregated_df = pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index=True)
    print(f"Aggregated data loaded with {len(aggregated_df)} rows.")

    # NEW STEP: Remove rows where there is a mismatch between PressedLetter and CurrentLetter (fixed column name)
    print("Removing rows with typos (where PressedLetter != CurrentLetter)...")
    aggregated_df = aggregated_df[aggregated_df['PressedLetter'] == aggregated_df['CurrentLetter']].copy()
    print(f"Remaining rows after typo removal: {len(aggregated_df)}")

    if len(aggregated_df) == 0:
      print("No valid data remaining after typo removal. Please check your dataset.")
      return None

    # Handle missing values with interpolation for all numerical columns
    print("Interpolating missing data...")
    numerical_cols = aggregated_df.select_dtypes(include=np.number).columns
    aggregated_df[numerical_cols] = aggregated_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[numerical_cols].transform(lambda x: x.interpolate(limit_direction='both'))

    # Drop rows that still have NaNs (e.g., at the beginning of a sequence)
    aggregated_df.dropna(inplace=True)

    # Store the original DataFrame to add new columns to later
    df = aggregated_df.copy()

    # Dictionary to hold all newly engineered features
    new_features = {}

    print("Step 2: Performing dynamic feature normalization...")

    # Use the average of the two wrist roots as the dynamic origin for each row.
    df['Origin_X'] = (df['Left_Hand_WristRoot_X'] + df['Right_Hand_WristRoot_X']) / 2
    df['Origin_Y'] = (df['Left_Hand_WristRoot_Y'] + df['Right_Hand_WristRoot_Y']) / 2
    df['Origin_Z'] = (df['Left_Hand_WristRoot_Z'] + df['Right_Hand_WristRoot_Z']) / 2

    # Identify all original coordinate columns that need normalization
    # This now filters out any already-normalized columns to prevent the KeyError.
    raw_coord_cols = [col for col in df.columns if ('_X' in col or '_Y' in col or '_Z' in col) and 'GazeRay' not in col and not col.startswith('normalized_')]
    key_cols = [col for col in raw_coord_cols if col.startswith('Key_')]

    # Create normalized columns in a single vectorized operation and add to new_features dict
    for col in raw_coord_cols:
        new_features[f'normalized_{col}'] = df[col] - df[f'Origin_{col[-1]}']

    # Add the origin columns to the DataFrame since they're needed for normalization
    df = df.assign(
        Origin_X = new_features.get('Origin_X', df['Origin_X']),
        Origin_Y = new_features.get('Origin_Y', df['Origin_Y']),
        Origin_Z = new_features.get('Origin_Z', df['Origin_Z'])
    )

    print("Step 3: Inferring the pressed finger based on proximity...")

    # Use the newly created normalized columns for inference
    normalized_df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    finger_tip_cols_x = [col for col in normalized_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    # Get a list of unique keys that actually have corresponding columns in the DataFrame
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in normalized_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]
    unique_pressed_letters = normalized_df['PressedLetter'].dropna().unique()

    # Filter the list of pressed letters to only include those with available key data
    processed_keys = [key for key in unique_pressed_letters if key in available_keys]

    all_distances = pd.DataFrame(index=normalized_df.index, columns=finger_tip_names, dtype=float)

    # UPDATED: Use 3D coordinates for proximity calculation
    for key in processed_keys:
        mask = (normalized_df['PressedLetter'] == key)
        if not mask.any():
            continue

        key_coords = normalized_df.loc[mask, [f'normalized_Key_{key}_X', f'normalized_Key_{key}_Y', f'normalized_Key_{key}_Z']].values

        for finger in finger_tip_names:
            finger_coords = normalized_df.loc[mask, [f'normalized_{finger}_X', f'normalized_{finger}_Y', f'normalized_{finger}_Z']].values
            dist_sq = ((finger_coords - key_coords)**2).sum(axis=1)
            all_distances.loc[mask, finger] = dist_sq

    # Use .idxmin with skipna=True to avoid the FutureWarning
    df['InferredPressedFinger'] = all_distances.idxmin(axis=1, skipna=True)
    df.dropna(subset=['InferredPressedFinger'], inplace=True)

    # Rebuild new_features dictionary with normalized and inferred data for the next steps
    temp_df = pd.concat([df, pd.DataFrame(new_features)], axis=1)
    normalized_cols = [col for col in temp_df.columns if col.startswith('normalized_')]

    print("Step 4: Creating delta features from normalized data...")
    for col in normalized_cols:
        new_features[f'delta_{col}'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[col].diff().fillna(0)

    print("Step 5: Engineering new geometric and kinematic features...")

    # Kinematic features: Hand velocity
    new_features['Left_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Left_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))
    new_features['Right_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Right_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))

    # Gaze-to-key distance features - IMPROVED to handle both Left and Right Gaze
    unique_keys = [col.split('_')[1] for col in key_cols if col.endswith('_X')]

    temp_df['GazeHitPosition_X'] = np.nan
    temp_df['GazeHitPosition_Y'] = np.nan
    temp_df['GazeHitPosition_Z'] = np.nan

    # Case 1: Both left and right gaze hit flags are 1 (using corrected column names)
    both_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 1)
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_X'] = (temp_df['LeftGazeHitPosition_X'] + temp_df['RightGazeHitPosition_X']) / 2
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_Y'] = (temp_df['LeftGazeHitPosition_Y'] + temp_df['RightGazeHitPosition_Y']) / 2
    temp_df.loc[both_gaze_hit, 'GazeHitPosition_Z'] = (temp_df['LeftGazeHitPosition_Z'] + temp_df['RightGazeHitPosition_Z']) / 2

    # Case 2: Only left gaze hit flag is 1 (using corrected column names)
    left_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 0)
    temp_df.loc[left_gaze_hit, ['GazeHitPosition_X', 'GazeHitPosition_Y', 'GazeHitPosition_Z']] = temp_df.loc[left_gaze_hit, ['LeftGazeHitPosition_X', 'LeftGazeHitPosition_Y', 'LeftGazeHitPosition_Z']].values

    # Case 3: Only right gaze hit flag is 1 (using corrected column names)
    right_gaze_hit = (temp_df['LeftGazeHit'] == 0) & (temp_df['RightGazeHit'] == 1)
    temp_df.loc[right_gaze_hit, ['GazeHitPosition_X', 'GazeHitPosition_Y', 'GazeHitPosition_Z']] = temp_df.loc[right_gaze_hit, ['RightGazeHitPosition_X', 'RightGazeHitPosition_Y', 'RightGazeHitPosition_Z']].values

    for key in unique_keys:
        key_x_col = f'normalized_Key_{key}_X'
        key_y_col = f'normalized_Key_{key}_Y'
        key_z_col = f'normalized_Key_{key}_Z'
        if key_x_col in temp_df.columns and key_y_col in temp_df.columns and key_z_col in temp_df.columns:
            new_features[f'gaze_dist_{key}'] = np.sqrt(
                (temp_df['GazeHitPosition_X'] - temp_df[key_x_col])**2 +
                (temp_df['GazeHitPosition_Y'] - temp_df[key_y_col])**2 +
                (temp_df['GazeHitPosition_Z'] - temp_df[key_z_col])**2
            )

    # Finger-to-key distance features (Vectorized and robust)
    available_keys = [col.replace('normalized_Key_', '').replace('_X', '') for col in temp_df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]

    pressed_key_x_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_y_series = pd.Series(index=temp_df.index, dtype=float)
    pressed_key_z_series = pd.Series(index=temp_df.index, dtype=float)

    for key in available_keys:
        mask = temp_df['PressedLetter'] == key
        if mask.any():
            pressed_key_x_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_X']
            pressed_key_y_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Y']
            pressed_key_z_series.loc[mask] = temp_df.loc[mask, f'normalized_Key_{key}_Z']

    finger_tip_cols_x = [col for col in temp_df.columns if 'Tip_X' in col and col.startswith('normalized_')]
    finger_tip_names = [col.replace('normalized_', '').replace('_X', '') for col in finger_tip_cols_x]

    for finger in finger_tip_names:
        finger_x_col = f'normalized_{finger}_X'
        finger_y_col = f'normalized_{finger}_Y'
        finger_z_col = f'normalized_{finger}_Z'

        distances = np.sqrt(
            (temp_df[finger_x_col] - pressed_key_x_series)**2 +
            (temp_df[finger_y_col] - pressed_key_y_series)**2 +
            (temp_df[finger_z_col] - pressed_key_z_series)**2
        )
        new_features[f'finger_dist_{finger}'] = distances

    # Concatenate all new features at once to the original df
    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)

    df.dropna(inplace=True)
    print("Data preprocessing complete.")
    return df

def create_padded_sequences(df, features, target_col):
    """
    Groups data and pads sequences for LSTM input.
    """
    sequences, labels, participants = [], [], []

    max_len = 0
    unique_groups = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])
    for _, group in unique_groups:
        max_len = max(max_len, len(group))

    if max_len > MAX_SEQUENCE_LENGTH:
        max_len = MAX_SEQUENCE_LENGTH

    for group_key, group in unique_groups:
        group_features = group[features].values
        padded_features = pad_sequences([group_features], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0]

        sequences.append(padded_features)
        labels.append(group[target_col].iloc[-1])
        participants.append(group_key[0])

    return np.array(sequences), np.array(labels), np.array(participants)

def build_model(input_shape, num_classes):
    """
    Builds and compiles the Bidirectional LSTM model with a Masking layer.
    """
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=input_shape))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def run_loocv_evaluation(df, features, target_col, stage_name):
    """
    Performs Leave-One-Out Cross-Validation on the provided data and returns metrics.
    """
    print(f"\n--- Running LOOCV for {stage_name} ---")

    X_sequences, y_labels, participant_ids = create_padded_sequences(df, features, target_col)

    if len(X_sequences) == 0:
      print("Skipping evaluation: No data available for this stage.")
      return None

    unique_participants = np.unique(participant_ids)
    participant_map = {p: i for i, p in enumerate(unique_participants)}
    participant_indices = np.array([participant_map[p] for p in participant_ids])

    if len(unique_participants) < 2:
        print("Skipping evaluation: Not enough participants for LOOCV.")
        return None

    le = LabelEncoder()
    y_encoded = le.fit_transform(y_labels)
    num_classes = len(le.classes_)
    y_categorical = to_categorical(y_encoded, num_classes=num_classes)

    print(f"Total samples for evaluation: {X_sequences.shape[0]}. Classes: {le.classes_}")

    loocv = LeaveOneOut()
    accuracy_scores, y_true_all, y_pred_all = [], [], []

    for train_participant_indices, test_participant_indices in loocv.split(unique_participants):
        train_participants = unique_participants[train_participant_indices]
        test_participant = unique_participants[test_participant_indices][0]

        train_indices = np.where(np.isin(participant_ids, train_participants))[0]
        test_indices = np.where(participant_ids == test_participant)[0]

        X_train, X_test = X_sequences[train_indices], X_sequences[test_indices]
        y_train, y_test = y_categorical[train_indices], y_categorical[test_indices]
        y_test_labels = np.argmax(y_test, axis=1)

        model = build_model(X_train.shape[1:], num_classes)
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)

        class_counts = pd.Series(y_train.argmax(axis=1)).value_counts()
        total_samples = len(y_train)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}

        history = model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            class_weight=class_weights
        )

        y_pred_probs = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracy_scores.append(accuracy)

        y_true_all.extend(y_test_labels)
        y_pred_all.extend(y_pred)

    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    final_report = classification_report(y_true_all, y_pred_all, target_names=le.classes_, zero_division=0)

    return avg_accuracy, final_report, le, y_true_all, y_pred_all

# Cell 3: Data Loading and Preprocessing

# This is where you would start the main execution.
full_df = load_and_preprocess_data(INPUT_FOLDER)
if full_df is None:
    # If the data loading fails, the script will stop here.
    # Otherwise, the 'full_df' DataFrame is ready for the next steps.
    pass
else:
    # Cell 4: Stage 1 - Finger Prediction
    print("--- Starting Stage 1: Finger Prediction ---")
    stage1_features = [
        col for col in full_df.columns if (
            col.startswith('normalized_') or
            col.startswith('delta_') or
            col.startswith('Left_Hand_WristRoot_Velocity') or
            col.startswith('Right_Hand_WristRoot_Velocity')
        ) and 'Key_' not in col and 'Gaze' not in col
    ]
    avg_stage1_accuracy, stage1_report, le_stage1, _, _ = run_loocv_evaluation(
        full_df,
        stage1_features,
        'InferredPressedFinger',
        "Stage 1: Finger Prediction"
    )

    print("\n==============================================")
    print("Stage 1 LOOCV Results: Finger Prediction")
    print(f"Average Accuracy: {avg_stage1_accuracy:.4f}")
    print("\nClassification Report:")
    print(stage1_report)
    print("==============================================\n")

    # Cell 5: Stage 2 - Character Prediction within each Finger
    print("--- Starting Stage 2: Character Prediction ---")
    finger_accuracies = {}
    unique_fingers = sorted(full_df['InferredPressedFinger'].unique())

    print("Stage 2: Character Prediction (LOOCV for each finger)")
    for finger_id in unique_fingers:
        finger_df = full_df[full_df['InferredPressedFinger'] == finger_id].copy()

        stage2_features = [
            col for col in finger_df.columns if (
                col.startswith('normalized_') or
                col.startswith('delta_') or
                col.startswith('gaze_dist_') or
                col.startswith('finger_dist_') or
                col.startswith('Left_Hand_WristRoot_Velocity') or
                col.startswith('Right_Hand_WristRoot_Velocity')
            )
        ]

        result = run_loocv_evaluation(
            finger_df,
            stage2_features,
            'PressedLetter',
            f"Stage 2, Finger {finger_id}"
        )

        if result:
            avg_accuracy, report, le, y_true, y_pred = result
            finger_accuracies[finger_id] = {
                'accuracy': avg_accuracy,
                'report': report,
                'le': le,
                'y_true': y_true,
                'y_pred': y_pred
            }
            print(f"\nAverage Accuracy for Finger {finger_id}: {avg_accuracy:.4f}")
            print(f"Classification Report for Finger {finger_id}:\n{report}")

    print("\n==============================================")
    print("Final Stage 2 Accuracy Report")
    for finger_id, metrics in finger_accuracies.items():
        print(f"Inferred Finger {finger_id} (Keys: {list(metrics['le'].classes_)}): {metrics['accuracy']:.4f}")
    print("==============================================")















# Cell 1: Imports and Configuration

import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import re

# --- CONFIG ---
MAX_SEQUENCE_LENGTH = 50
EPOCHS = 40
BATCH_SIZE = 32
PATIENCE = 5 # For EarlyStopping

INPUT_FOLDER = "/content/extracted_participants" # Updated to reflect your extraction path

# Cell 2: Script Functions

def load_and_preprocess_data(folder_path):
    """
    Loads all data, handles missing values, and performs dynamic feature engineering
    with a robust, non-fragmenting approach. This version is simplified to handle
    the absence of key coordinate data.
    """
    print("Step 1: Loading and preprocessing data...")
    all_csv_files = glob.glob(os.path.join(folder_path, "**/*.csv"), recursive=True)
    if not all_csv_files:
        print("Error: No CSV files found.")
        return None

    aggregated_df = pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index=True)
    print(f"Aggregated data loaded with {len(aggregated_df)} rows.")

    print("Removing rows with typos (where PressedLetter != CurrentLetter)...")
    df = aggregated_df[aggregated_df['PressedLetter'] == aggregated_df['CurrentLetter']].copy()
    print(f"Remaining rows after typo removal: {len(df)}")

    if len(df) == 0:
      print("No valid data remaining after typo removal. Please check your dataset.")
      return None

    print("Interpolating missing data...")
    numerical_cols = df.select_dtypes(include=np.number).columns
    df[numerical_cols] = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[numerical_cols].transform(lambda x: x.interpolate(limit_direction='both'))
    df.dropna(inplace=True)

    # Store all new features in a single dictionary to avoid fragmentation
    new_features_dict = {}

    print("Step 2: Performing dynamic feature normalization...")

    # Calculate origin and add to dictionary
    new_features_dict['Origin_X'] = (df['Left_Hand_WristRoot_X'] + df['Right_Hand_WristRoot_X']) / 2
    new_features_dict['Origin_Y'] = (df['Left_Hand_WristRoot_Y'] + df['Right_Hand_WristRoot_Y']) / 2
    new_features_dict['Origin_Z'] = (df['Left_Hand_WristRoot_Z'] + df['Right_Hand_WristRoot_Z']) / 2

    raw_coord_cols = [col for col in df.columns if ('_X' in col or '_Y' in col or '_Z' in col) and 'GazeRay' not in col and not col.startswith('normalized_')]

    # Normalize coordinates for all hands
    for col in raw_coord_cols:
      if 'Key' not in col:
        new_features_dict[f'normalized_{col}'] = df[col] - new_features_dict[f'Origin_{col[-1]}']

    # --- Step 3: Finger inference is now removed as key coordinates are missing. ---
    print("Step 3: Skipping finger inference due to missing key coordinate data.")

    # We will now work directly with the normalized features and other existing data
    temp_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(new_features_dict, index=df.index).reset_index(drop=True)], axis=1)

    temp_df.dropna(inplace=True)

    if temp_df.empty:
        print("Final DataFrame is empty after all preprocessing. Returning None.")
        return None

    print(f"Rows remaining after preprocessing: {len(temp_df)}")

    print("Step 4: Creating delta features from normalized data...")
    normalized_cols = [col for col in temp_df.columns if col.startswith('normalized_')]
    for col in normalized_cols:
        new_features_dict[f'delta_{col}'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[col].diff().fillna(0)

    print("Step 5: Engineering new geometric and kinematic features...")

    new_features_dict['Left_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Left_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))
    new_features_dict['Right_Hand_WristRoot_Velocity'] = temp_df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Right_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))

    # Gaze features are still usable
    gaze_hit_x = np.full(len(temp_df), np.nan)
    gaze_hit_y = np.full(len(temp_df), np.nan)
    gaze_hit_z = np.full(len(temp_df), np.nan)

    both_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 1)
    gaze_hit_x[both_gaze_hit] = (temp_df.loc[both_gaze_hit, 'LeftGazeHitPosition_X'] + temp_df.loc[both_gaze_hit, 'RightGazeHitPosition_X']) / 2
    gaze_hit_y[both_gaze_hit] = (temp_df.loc[both_gaze_hit, 'LeftGazeHitPosition_Y'] + temp_df.loc[both_gaze_hit, 'RightGazeHitPosition_Y']) / 2
    gaze_hit_z[both_gaze_hit] = (temp_df.loc[both_gaze_hit, 'LeftGazeHitPosition_Z'] + temp_df.loc[both_gaze_hit, 'RightGazeHitPosition_Z']) / 2

    left_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 0)
    gaze_hit_x[left_gaze_hit] = temp_df.loc[left_gaze_hit, 'LeftGazeHitPosition_X']
    gaze_hit_y[left_gaze_hit] = temp_df.loc[left_gaze_hit, 'LeftGazeHitPosition_Y']
    gaze_hit_z[left_gaze_hit] = temp_df.loc[left_gaze_hit, 'LeftGazeHitPosition_Z']

    right_gaze_hit = (temp_df['LeftGazeHit'] == 1) & (temp_df['RightGazeHit'] == 1)
    gaze_hit_x[right_gaze_hit] = temp_df.loc[right_gaze_hit, 'RightGazeHitPosition_X']
    gaze_hit_y[right_gaze_hit] = temp_df.loc[right_gaze_hit, 'RightGazeHitPosition_Y']
    gaze_hit_z[right_gaze_hit] = temp_df.loc[right_gaze_hit, 'RightGazeHitPosition_Z']

    # Use a single concat call to avoid fragmentation warnings and ensure data integrity
    df = pd.concat([temp_df.reset_index(drop=True), pd.DataFrame(new_features_dict, index=temp_df.index).reset_index(drop=True)], axis=1)

    df.dropna(inplace=True)
    print("Data preprocessing complete.")
    if df.empty:
        print("Final DataFrame is empty after all preprocessing. Returning None.")
        return None
    return df

def create_padded_sequences(df, features, target_col):
    """
    Groups data and pads sequences for LSTM input.
    """
    sequences, labels, participants = [], [], []

    max_len = 0
    unique_groups = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])
    for _, group in unique_groups:
        max_len = max(max_len, len(group))

    if max_len > MAX_SEQUENCE_LENGTH:
        max_len = MAX_SEQUENCE_LENGTH

    for group_key, group in unique_groups:
        group_features = group[features].values
        padded_features = pad_sequences([group_features], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0]

        sequences.append(padded_features)
        labels.append(group[target_col].iloc[-1])
        participants.append(group_key[0])

    return np.array(sequences), np.array(labels), np.array(participants)

def build_model(input_shape, num_classes):
    """
    Builds and compiles the Bidirectional LSTM model with a Masking layer.
    """
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=input_shape))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def run_loocv_evaluation(df, features, target_col, stage_name):
    """
    Performs Leave-One-Out Cross-Validation on the provided data and returns metrics.
    This version now works for a single-stage model.
    """
    print(f"\n--- Running LOOCV for {stage_name} ---")

    X_sequences, y_labels, participant_ids = create_padded_sequences(df, features, target_col)

    if len(X_sequences) == 0:
      print("Skipping evaluation: No data available for this stage.")
      return None

    unique_participants = np.unique(participant_ids)
    participant_map = {p: i for i, p in enumerate(unique_participants)}
    participant_indices = np.array([participant_map[p] for p in participant_ids])

    if len(unique_participants) < 2:
        print("Skipping evaluation: Not enough participants for LOOCV.")
        return None

    le = LabelEncoder()
    y_encoded = le.fit_transform(y_labels)
    num_classes = len(le.classes_)
    y_categorical = to_categorical(y_encoded, num_classes=num_classes)

    print(f"Total samples for evaluation: {X_sequences.shape[0]}. Classes: {le.classes_}")

    loocv = LeaveOneOut()
    accuracy_scores, y_true_all, y_pred_all = [], [], []

    for train_participant_indices, test_participant_indices in loocv.split(unique_participants):
        train_participants = unique_participants[train_participant_indices]
        test_participant = unique_participants[test_participant_indices][0]

        train_indices = np.where(np.isin(participant_ids, train_participants))[0]
        test_indices = np.where(participant_ids == test_participant)[0]

        X_train, X_test = X_sequences[train_indices], X_sequences[test_indices]
        y_train, y_test = y_categorical[train_indices], y_categorical[test_indices]
        y_test_labels = np.argmax(y_test, axis=1)

        model = build_model(X_train.shape[1:], num_classes)
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)

        class_counts = pd.Series(y_train.argmax(axis=1)).value_counts()
        total_samples = len(y_train)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}

        history = model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            class_weight=class_weights
        )

        y_pred_probs = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracy_scores.append(accuracy)

        y_true_all.extend(y_test_labels)
        y_pred_all.extend(y_pred)

    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    final_report = classification_report(y_true_all, y_pred_all, target_names=le.classes_, zero_division=0)

    return avg_accuracy, final_report, le, y_true_all, y_pred_all


# Cell 3: Data Loading and Preprocessing

# This is where you would start the main execution.
full_df = load_and_preprocess_data(INPUT_FOLDER)
if full_df is None:
    # If the data loading fails, the script will stop here.
    pass
else:
    # Cell 4: Single-Stage Prediction
    print("--- Starting Single-Stage Prediction ---")

    # We now combine all available features to predict the PressedLetter directly.
    all_features = [
        col for col in full_df.columns if (
            col.startswith('normalized_') or
            col.startswith('delta_') or
            col.startswith('Left_Hand_WristRoot_Velocity') or
            col.startswith('Right_Hand_WristRoot_Velocity') or
            col.startswith('LeftGazeHitPosition') or
            col.startswith('RightGazeHitPosition')
        )
    ]

    avg_accuracy, report, le, _, _ = run_loocv_evaluation(
        full_df,
        all_features,
        'PressedLetter',
        "Single-Stage: Character Prediction"
    )

    print("\n==============================================")
    print("Single-Stage LOOCV Results: Character Prediction")
    print(f"Average Accuracy: {avg_accuracy:.4f}")
    print("\nClassification Report:")
    print(report)
    print("==============================================\n")













# Cell 1: Imports and Configuration

import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import re

# --- CONFIG ---
MAX_SEQUENCE_LENGTH = 50
EPOCHS = 40
BATCH_SIZE = 32
PATIENCE = 5 # For EarlyStopping

INPUT_FOLDER = "/content/extracted_participants"

# Note: We no longer use hardcoded FINGER_MAPPING or KEY_LAYOUT_RELATIVE
# as these are now inferred or read directly from the data.

# Explicit, correct list of finger tips
FINGER_TIPS = [
    'Left_Hand_ThumbTip', 'Left_Hand_IndexTip', 'Left_Hand_MiddleTip', 'Left_Hand_RingTip', 'Left_Hand_PinkyTip',
    'Right_Hand_ThumbTip', 'Right_Hand_IndexTip', 'Right_Hand_MiddleTip', 'Right_Hand_RingTip', 'Right_Hand_PinkyTip'
]


# Cell 2: Script Functions

def load_and_preprocess_data(folder_path):
    """
    Loads all data, handles missing values, and performs dynamic feature engineering
    in a robust and performant way.
    """
    print("Step 1: Loading and preprocessing data...")
    all_csv_files = glob.glob(os.path.join(folder_path, "**/*.csv"), recursive=True)
    if not all_csv_files:
        print("Error: No CSV files found.")
        return None

    # Concatenate all files into a single DataFrame
    aggregated_df = pd.concat([pd.read_csv(f) for f in all_csv_files], ignore_index=True)
    print(f"Aggregated data loaded with {len(aggregated_df)} rows.")

    print("Removing rows with typos (where PressedLetter != CurrentLetter)...")
    df = aggregated_df[aggregated_df['PressedLetter'] == aggregated_df['CurrentLetter']].copy()
    print(f"Remaining rows after typo removal: {len(df)}")

    if len(df) == 0:
      print("No valid data remaining after typo removal. Please check your dataset.")
      return None

    # Handle missing values with interpolation for all numerical columns
    print("Interpolating missing data...")
    numerical_cols = df.select_dtypes(include=np.number).columns
    df[numerical_cols] = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[numerical_cols].transform(lambda x: x.interpolate(limit_direction='both'))

    # Drop rows that still have NaNs (e.g., at the beginning of a sequence)
    df.dropna(inplace=True)

    if df.empty:
        print("DataFrame is empty after interpolation and dropping NaNs. Returning None.")
        return None

    # Initialize dictionaries for all new features to be added later
    new_features_dict = {}

    print("Step 2: Performing dynamic feature normalization...")

    # Use the average of the two wrist roots as the dynamic origin for each row.
    df['Origin_X'] = (df['Left_Hand_WristRoot_X'] + df['Right_Hand_WristRoot_X']) / 2
    df['Origin_Y'] = (df['Left_Hand_WristRoot_Y'] + df['Right_Hand_WristRoot_Y']) / 2
    df['Origin_Z'] = (df['Left_Hand_WristRoot_Z'] + df['Right_Hand_WristRoot_Z']) / 2

    # Identify all coordinate columns that need normalization
    coord_cols = [col for col in df.columns if ('_X' in col or '_Y' in col or '_Z' in col) and 'GazeRay' not in col and 'Origin_' not in col]

    # Create normalized columns using vectorized operations
    for col in coord_cols:
        axis = col[-1]
        new_features_dict[f'normalized_{col}'] = df[col] - df[f'Origin_{axis}']

    df = df.assign(**new_features_dict)

    print("Step 3: Inferring the pressed finger based on proximity...")

    # Pre-calculate distances from each finger tip to the dynamically pressed key
    distance_data = {}
    for finger in FINGER_TIPS:
        finger_x_col = f'normalized_{finger}_X'
        finger_y_col = f'normalized_{finger}_Y'

        # Safely get the normalized coordinates of the currently pressed key
        pressed_key_x_series = df.apply(lambda row: row.get(f'normalized_Key_{row["PressedLetter"]}_X', np.nan), axis=1)
        pressed_key_y_series = df.apply(lambda row: row.get(f'normalized_Key_{row["PressedLetter"]}_Y', np.nan), axis=1)

        distance_data[finger] = np.sqrt(
            (df[finger_x_col] - pressed_key_x_series)**2 +
            (df[finger_y_col] - pressed_key_y_series)**2
        )

    distances_df = pd.DataFrame(distance_data, index=df.index)
    df['InferredPressedFinger'] = distances_df.idxmin(axis=1)

    df.dropna(subset=['InferredPressedFinger'], inplace=True)

    # Check if the DataFrame is still empty
    if df.empty:
      print("DataFrame is empty after finger inference and dropping NaNs. Returning None.")
      return None

    print("Step 4: Creating delta features from normalized data...")
    normalized_cols = [col for col in df.columns if col.startswith('normalized_')]

    delta_dict = {
        f'delta_{col}': df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])[col].diff().fillna(0) for col in normalized_cols
    }
    df = pd.concat([df, pd.DataFrame(delta_dict, index=df.index)], axis=1)

    print("Step 5: Engineering new geometric and kinematic features...")

    new_features_final = {}
    # Kinematic features: Hand velocity
    new_features_final['Left_Hand_WristRoot_Velocity'] = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Left_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))
    new_features_final['Right_Hand_WristRoot_Velocity'] = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])['normalized_Right_Hand_WristRoot_X'].transform(lambda x: x.diff().fillna(0))

    # Gaze-to-key distance features
    normalized_gaze_x = 'normalized_LeftGazeHitPosition_X'
    normalized_gaze_y = 'normalized_LeftGazeHitPosition_Y'
    unique_keys = [re.search(r'Key_(.+)_X', col).group(1) for col in df.columns if col.startswith('normalized_Key_') and col.endswith('_X')]

    for key in unique_keys:
        key_x_col = f'normalized_Key_{key}_X'
        key_y_col = f'normalized_Key_{key}_Y'
        if key_x_col in df.columns and key_y_col in df.columns and normalized_gaze_x in df.columns:
            new_features_final[f'gaze_dist_{key}'] = np.sqrt(
                (df[normalized_gaze_x] - df[key_x_col])**2 +
                (df[normalized_gaze_y] - df[key_y_col])**2
            )

    # Finger-to-key distance features are already calculated in the distance_data df
    df = pd.concat([df, pd.DataFrame(new_features_final, index=df.index), distances_df], axis=1)

    df.dropna(inplace=True)
    print("Data preprocessing complete.")
    return df

def create_padded_sequences(df, features, target_col):
    """
    Groups data and pads sequences for LSTM input.
    """
    sequences, labels, participants = [], [], []

    max_len = 0
    unique_groups = df.groupby(['ParticipantID', 'TrialNumber', 'LetterIndex'])
    for _, group in unique_groups:
        max_len = max(max_len, len(group))

    if max_len > MAX_SEQUENCE_LENGTH:
        max_len = MAX_SEQUENCE_LENGTH

    for group_key, group in unique_groups:
        group_features = group[features].values

        padded_features = pad_sequences([group_features], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0]

        sequences.append(padded_features)
        labels.append(group[target_col].iloc[-1])
        participants.append(group_key[0])

    return np.array(sequences), np.array(labels), np.array(participants)

def build_model(input_shape, num_classes):
    """
    Builds and compiles the Bidirectional LSTM model with a Masking layer.
    """
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=input_shape))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def run_loocv_evaluation(df, features, target_col, stage_name):
    """
    Performs Leave-One-Out Cross-Validation on the provided data and returns metrics.
    """
    print(f"\n--- Running LOOCV for {stage_name} ---")

    # Explicitly check for data before creating sequences
    if df.empty:
      print("Skipping evaluation: The DataFrame is empty.")
      return None

    X_sequences, y_labels, participant_ids = create_padded_sequences(df, features, target_col)

    if len(X_sequences) == 0:
      print("Skipping evaluation: No data available for this stage after sequence creation.")
      return None

    unique_participants = np.unique(participant_ids)

    if len(unique_participants) < 2:
        print("Skipping evaluation: Not enough participants for LOOCV.")
        return None

    le = LabelEncoder()
    y_encoded = le.fit_transform(y_labels)
    num_classes = len(le.classes_)
    y_categorical = to_categorical(y_encoded, num_classes=num_classes)

    print(f"Total samples for evaluation: {X_sequences.shape[0]}. Classes: {le.classes_}")

    loocv = LeaveOneOut()
    accuracy_scores, y_true_all, y_pred_all = [], [], []

    for train_participant_indices, test_participant_indices in loocv.split(unique_participants):
        train_participants = unique_participants[train_participant_indices]
        test_participant = unique_participants[test_participant_indices][0]

        train_indices = np.where(np.isin(participant_ids, train_participants))[0]
        test_indices = np.where(participant_ids == test_participant)[0]

        X_train, X_test = X_sequences[train_indices], X_sequences[test_indices]
        y_train, y_test = y_categorical[train_indices], y_categorical[test_indices]
        y_test_labels = np.argmax(y_test, axis=1)

        model = build_model(X_train.shape[1:], num_classes)
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)

        class_counts = pd.Series(y_train.argmax(axis=1)).value_counts()
        total_samples = len(y_train)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}

        history = model.fit(
            X_train, y_train,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            class_weight=class_weights
        )

        y_pred_probs = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracy_scores.append(accuracy)

        y_true_all.extend(y_test_labels)
        y_pred_all.extend(y_pred)

    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    final_report = classification_report(y_true_all, y_pred_all, target_names=le.classes_, zero_division=0)

    return avg_accuracy, final_report, le, y_true_all, y_pred_all

# Cell 3: Main Script Execution
if __name__ == "__main__":

    full_df = load_and_preprocess_data(INPUT_FOLDER)
    if full_df is None:
        exit()

    # --- Stage 1: Finger Prediction ---
    # Explicitly define the features to avoid errors
    stage1_features = [col for col in full_df.columns if (
        col.startswith('normalized_') or
        col.startswith('delta_')
    ) and 'Key_' not in col and 'Gaze' not in col and 'finger_dist' not in col]
    stage1_features.extend([
        'Left_Hand_WristRoot_Velocity',
        'Right_Hand_WristRoot_Velocity'
    ])

    avg_stage1_accuracy, stage1_report, le_stage1, _, _ = run_loocv_evaluation(
        full_df,
        stage1_features,
        'InferredPressedFinger',
        "Stage 1: Finger Prediction"
    )

    if avg_stage1_accuracy is not None:
        print("\n==============================================")
        print("Stage 1 LOOCV Results: Finger Prediction")
        print(f"Average Accuracy: {avg_stage1_accuracy:.4f}")
        print("\nClassification Report:")
        print(stage1_report)
        print("==============================================\n")

    # --- Stage 2: Character Prediction within each Finger ---
    finger_accuracies = {}
    unique_fingers = sorted(full_df['InferredPressedFinger'].unique())

    print("Stage 2: Character Prediction (LOOCV for each finger)")
    for finger_id in unique_fingers:
        finger_df = full_df[full_df['InferredPressedFinger'] == finger_id].copy()

        # Explicitly define the features for stage 2
        stage2_features = [col for col in finger_df.columns if (
            col.startswith('normalized_') or
            col.startswith('delta_') or
            col.startswith('gaze_dist_') or
            col.startswith('finger_dist_')
        )]
        stage2_features.extend([
            'Left_Hand_WristRoot_Velocity',
            'Right_Hand_WristRoot_Velocity'
        ])

        if not stage2_features:
            print(f"No features available for Finger {finger_id}. Skipping.")
            continue

        result = run_loocv_evaluation(
            finger_df,
            stage2_features,
            'PressedLetter',
            f"Stage 2, Finger {finger_id}"
        )

        if result:
            avg_accuracy, report, le, y_true, y_pred = result
            finger_accuracies[finger_id] = {
                'accuracy': avg_accuracy,
                'report': report,
                'le': le,
                'y_true': y_true,
                'y_pred': y_pred
            }
            print(f"\nAverage Accuracy for Finger {finger_id}: {avg_accuracy:.4f}")
            print(f"Classification Report for Finger {finger_id}:\n{report}")

    print("\n==============================================")
    print("Final Stage 2 Accuracy Report")
    for finger_id, metrics in finger_accuracies.items():
        # Using a safer way to get the class names, as some clusters may not have all letters
        try:
            class_names = list(metrics['le'].classes_)
        except:
            class_names = ["N/A"]

        print(f"Inferred Finger {finger_id} (Keys: {class_names}): {metrics['accuracy']:.4f}")
    print("==============================================")

